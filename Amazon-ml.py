# -*- coding: utf-8 -*-
"""Untitled39.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OqKAQGLZeGrwLL3JuAik_LFaPl3iL58O
"""

import re
# import constants  This line is no longer needed
import os
import requests
import pandas as pd
import multiprocessing
import time
from time import time as timer
from tqdm import tqdm
import numpy as np
from pathlib import Path
from functools import partial
import requests
import urllib
from PIL import Image

# Add this line to define allowed_units if it doesn't exist elsewhere
allowed_units = ["meter", "foot", "centimeter", "millimeter", "inch"]

def common_mistake(unit):
    if unit in allowed_units: # Use allowed_units directly
        return unit
    if unit.replace('ter', 'tre') in allowed_units: # Use allowed_units directly
        return unit.replace('ter', 'tre')
    if unit.replace('feet', 'foot') in allowed_units: # Use allowed_units directly
        return unit.replace('feet', 'foot')
    return unit

def parse_string(s):
    s_stripped = "" if s==None or str(s)=='nan' else s.strip()
    if s_stripped == "":
        return None, None
    pattern = re.compile(r'^-?\d+(\.\d+)?\s+[a-zA-Z\s]+') # Fixed the regex pattern
    if not pattern.match(s_stripped):
        raise ValueError("Invalid format in {}".format(s))
    parts = s_stripped.split(maxsplit=1)
    number = float(parts[0])
    unit = common_mistake(parts[1])
    if unit not in allowed_units: # Use allowed_units directly
        raise ValueError("Invalid unit [{}] found in {}. Allowed units: {}".format(
            unit, s, allowed_units)) # Use allowed_units directly
    return number, unit


def create_placeholder_image(image_save_path):
    try:
        placeholder_image = Image.new('RGB', (100, 100), color='black')
        placeholder_image.save(image_save_path)
    except Exception as e:
        return

def download_image(image_link, save_folder, retries=3, delay=3):
    if not isinstance(image_link, str):
        return

    filename = Path(image_link).name
    image_save_path = os.path.join(save_folder, filename)

    if os.path.exists(image_save_path):
        return

    for _ in range(retries):
        try:
            urllib.request.urlretrieve(image_link, image_save_path)
            return
        except:
            time.sleep(delay)

    create_placeholder_image(image_save_path) #Create a black placeholder image for invalid links/images

def download_images(image_links, download_folder, allow_multiprocessing=True):
    if not os.path.exists(download_folder):
        os.makedirs(download_folder)

    if allow_multiprocessing:
        download_image_partial = partial(
            download_image, save_folder=download_folder, retries=3, delay=3)

        with multiprocessing.Pool(64) as pool:
            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))
            pool.close()
            pool.join()
    else:
        for image_link in tqdm(image_links, total=len(image_links)):
            download_image(image_link, save_folder=download_folder, retries=3, delay=3)





import os
import requests
from tqdm import tqdm  # For progress bar
import pandas as pd

# Load your dataset
df = pd.read_csv('/content/test.csv')  # Replace with the path to your CSV

# Create a folder to store downloaded images
image_dir = 'downloaded_images'
if not os.path.exists(image_dir):
    os.makedirs(image_dir)

# Function to download an image from a URL
def download_image(url, save_path):
    try:
        img_data = requests.get(url, timeout=10).content
        with open(save_path, 'wb') as handler:
            handler.write(img_data)
    except Exception as e:
        print(f"Failed to download {url}: {e}")

# Get the first 2,000 rows of the dataset
df_subset = df.head(2000)

# Download images using URLs from the subset of the dataset
for idx, row in tqdm(df_subset.iterrows(), total=df_subset.shape[0]):
    image_url = row['image_link']
    # Extract image name from URL and define the path to save the image
    image_name = image_url.split('/')[-1]
    image_path = os.path.join(image_dir, image_name)

    # Download the image
    download_image(image_url, image_path)

print("First 2,000 images downloaded!")

!pip install easyocr

!apt-get update
!apt-get install -y tesseract-ocr
!apt-get install -y libtesseract-dev
!apt-get install -y tesseract-ocr-eng

import os
import cv2
import pytesseract
import easyocr
import gc
import re
import time
import csv

# Initialize EasyOCR reader
reader = easyocr.Reader(['en'], gpu=False)

# Define allowed units
ALLOWED_UNITS = ['gram', 'kilogram', 'ounce', 'centimeter', 'inch','Meter','cm','KG','L','Lumens','Watts']

# Preprocess the image for better OCR performance
def preprocess_image(image_path):
    # Load the image
    image = cv2.imread(image_path)

    # Resize if the image is large (width > 1000 pixels)
    height, width = image.shape[:2]
    if width > 1000:
        scaling_factor = 1000 / width
        image = cv2.resize(image, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)

    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply thresholding to make text stand out
    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)

    return thresh

# Function to extract text from image using pytesseract or easyocr
def extract_text_from_image(image_path, method='pytesseract'):
    try:
        image = preprocess_image(image_path)

        if method == 'pytesseract':
            # Extract text using pytesseract
            text = pytesseract.image_to_string(image)
        elif method == 'easyocr':
            # Extract text using EasyOCR
            text = reader.readtext(image_path, detail=0)  # Direct path input
            text = ' '.join(text)  # Join the text from EasyOCR output

        return text
    except Exception as e:
        print(f"Error extracting text from {image_path}: {e}")
        return None
    finally:
        # Explicitly clear unused memory
        del image
        gc.collect()

# Function to parse entity value from text (e.g., weight, length)
def parse_entity_value(text):
    """
    Parse entity value (number + unit) from the given text based on allowed units.

    :param text: The text containing the entity value.
    :return: List of parsed values and units.
    """
    # Simple regex to find numbers followed by a unit from ALLOWED_UNITS
    pattern = r'(\d+(\.\d+)?)\s*(' + '|'.join(ALLOWED_UNITS) + ')'
    matches = re.findall(pattern, text)

    # Return list of matched entities (value + unit)
    return [(float(value), unit) for value, _, unit in matches]

# Function to write the parsed data to a CSV file
def write_to_csv(data, output_csv):
    """
    Write parsed data to a CSV file.

    :param data: List of tuples containing image filename, value, and unit.
    :param output_csv: Output CSV file path.
    """
    with open(output_csv, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Image File', 'Value', 'Unit'])  # Write header

        for row in data:
            writer.writerow(row)  # Write parsed data

# Process images in batches to avoid overloading system memory
def process_images_in_batches(image_folder, output_csv, batch_size=1000, method='pytesseract'):
    # Get a list of valid image files in the folder
    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]

    parsed_data = []  # To store parsed data from images

    # Process images in batches
    for i in range(0, len(image_files), batch_size):
        batch = image_files[i:i + batch_size]
        print(f"Processing batch {i // batch_size + 1}:")

        for image_file in batch:
            image_path = os.path.join(image_folder, image_file)

            # Skip large images (e.g., > 5 MB) to prevent overload
            if os.path.getsize(image_path) > 5 * 1024 * 1024:
                print(f"Skipping {image_file}: File too large")
                continue

            # Extract text from the image
            text_output = extract_text_from_image(image_path, method)
            if text_output:
                # Parse entity values (e.g., weight, length)
                parsed_values = parse_entity_value(text_output)

                # Append parsed data with image file name
                for value, unit in parsed_values:
                    parsed_data.append((image_file, value, unit))

        # Sleep for a few seconds between batches to prevent overload
        time.sleep(3)

    # Write all parsed data to the CSV file
    write_to_csv(parsed_data, output_csv)
    print(f"Parsed data has been written to {output_csv}")

# Specify the folder where the images are located and the CSV file for output
image_folder = '/content/downloaded_images'
output_csv = '/content/parsed_entity_values.csv'

# Start processing the images with a batch size of 10 and save results to CSV
process_images_in_batches(image_folder, output_csv, batch_size=1000, method='pytesseract')

from google.colab import files
files.download('/content/parsed_entity_values.csv')

import pandas as pd
import os

def load_parsed_data(parsed_csv):
    """
    Load the parsed data CSV containing image filenames, values, and units.
    """
    parsed_df = pd.read_csv(parsed_csv)
    parsed_df['filename'] = parsed_df['Image File'].apply(lambda x: x.split('/')[-1])  # Extract just the filename
    return parsed_df

def predictor(image_link, parsed_df):
    """
    Fetch the parsed prediction for the given image_link.
    """
    filename = image_link.split('/')[-1]  # Extract just the filename from the image_link

    # Find matching row in parsed data
    match = parsed_df[parsed_df['filename'] == filename]

    if match.empty:
        return ""  # If no match found, return an empty prediction
    else:
        value = match.iloc[0]['Value']
        unit = match.iloc[0]['Unit']
        return f"{value} {unit}"

if __name__ == "_main_":
    DATASET_FOLDER = '../dataset/'  # Adjust this as necessary
    PARSED_CSV = '/content/parsed_entity_values.csv'  # Path to your parsed entity CSV

    # Load the test CSV
    test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))

    # Load the parsed entity values
    parsed_df = load_parsed_data(PARSED_CSV)

    # Apply the predictor function for each row in the test CSV
    test['prediction'] = test.apply(lambda row: predictor(row['image_link'], parsed_df), axis=1)

    # Save the output predictions in the required format
    output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')
    test[['index', 'prediction']].to_csv(output_filename, index=False)

    print(f"Predictions saved to {output_filename}")

import pandas as pd
from sklearn.metrics import f1_score

# Load the data
parsed_df = pd.read_csv('//content/parsed_entity_values.csv')
test_out_df = pd.read_csv('/content/test_out.csv')

# Combine 'Value' and 'Unit' into a single string for true values
parsed_df['true_value'] = parsed_df['Value'].astype(str) + " " + parsed_df['Unit']

# Only use rows with non-NaN predictions
valid_test_df = test_out_df.dropna(subset=['prediction']).reset_index(drop=True)

# Also reset the parsed_df index for alignment
parsed_df = parsed_df.reset_index(drop=True)

# Now, compare only the rows that have predictions, and ensure their lengths match
min_length = min(len(parsed_df), len(valid_test_df))

# Slice both DataFrames to the same length
true_values = parsed_df.loc[:min_length-1, 'true_value']
pred_values = valid_test_df.loc[:min_length-1, 'prediction']

# Calculate F1 score (macro-average)
f1 = f1_score(true_values, pred_values, average='macro')
print(f"F1 Score: {f1}")
